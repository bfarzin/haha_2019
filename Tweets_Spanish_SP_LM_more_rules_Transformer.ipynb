{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.callbacks import SaveModelCallback\n",
    "\n",
    "import sentencepiece as spm #https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code Examples (testing out tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./data/rnn/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See RNN notebook for DataBunch generation.  It is the same for all LMs, so just loaded here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Piece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Line char for replacement in text\n",
    "NL = 'xxnl'\n",
    "defaults.text_spec_tok.append(NL) #add a New Line special char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_nl(t:str) -> str:\n",
    "    \"Replaces \\n by xxnl\"\n",
    "    return t.replace(\"\\r\\n\",\"\\n\").replace(\"\\n\",NL+\" \")\n",
    "\n",
    "def spec_add_spaces(t:str) -> str:\n",
    "    \"Add spaces between special characters\"\n",
    "    return re.sub(r'([/#?!@,])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t:str) -> str:\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t:str) -> str:\n",
    "    \"Replace repetitions at the character level\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t:str) -> str:\n",
    "    \"Replace word repetitions\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def deal_caps(t:str) -> str:\n",
    "    \"Replace words in all caps\"\n",
    "    res = []\n",
    "    for s in re.findall(r'\\w+|\\W+', t):\n",
    "        res += ([f' {TK_UP} ',s.lower()] if (s.isupper() and (len(s)>2)) else [s.lower()])\n",
    "    return ''.join(res)\n",
    "\n",
    "def fixup(x:str) -> str:\n",
    "    \"List of replacements from html strings\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "default_rules = [fixup, replace_rep, replace_wrep, deal_caps, spec_add_spaces, \n",
    "                 rm_useless_spaces, sub_nl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = 'all_tweets_es_0509'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## itos from m.vocab file: just read directly and populate the dictionary\n",
    "itos = [] #{}\n",
    "with open(f'{model_prefix}.vocab','r') as f:\n",
    "    for line_num,line in enumerate(f):\n",
    "        itos.append(line.split(\"\\t\")[0])\n",
    "\n",
    "class SPTokenizer(BaseTokenizer):\n",
    "    \"Wrapper around a SentncePiece tokenizer to make it a `BaseTokenizer`.\"\n",
    "    def __init__(self, model_prefix:str):\n",
    "        self.tok = spm.SentencePieceProcessor()\n",
    "        self.tok.load(f'{model_prefix}.model')\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return self.tok.EncodeAsPieces(t)\n",
    "    \n",
    "class CustomTokenizer():\n",
    "    '''Wrapper for SentencePiece toeknizer to fit into Fast.ai V1'''\n",
    "    def __init__(self,tok_func:Callable,model_prefix:str, pre_rules:ListRules=None):\n",
    "        self.tok_func,self.model_prefix = tok_func,model_prefix\n",
    "        self.pre_rules  = ifnone(pre_rules,  defaults.text_pre_rules )\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        res = f'Tokenizer {self.tok_func.__name__} using `{self.model_prefix}` model with the following rules:\\n'\n",
    "        for rule in self.pre_rules: res += f' - {rule.__name__}\\n'\n",
    "        return res        \n",
    "\n",
    "    def process_text(self, t:str,tok:BaseTokenizer) -> List[str]:\n",
    "        \"Processe one text `t` with tokenizer `tok`.\"\n",
    "        for rule in self.pre_rules: t = rule(t)  \n",
    "        toks = tok.tokenizer(t)\n",
    "        #post rules?\n",
    "        return toks \n",
    "    \n",
    "    def _process_all_1(self,texts:Collection[str]) -> List[List[str]]:\n",
    "        'Process a list of `texts` in one process'\n",
    "        tok = self.tok_func(self.model_prefix)\n",
    "        return [self.process_text(t,tok) for t in texts]\n",
    "                                                                     \n",
    "    def process_all(self, texts:Collection[str]) -> List[List[str]]: \n",
    "        \"Process a list of `texts`.\"                                 \n",
    "        return self._process_all_1(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycust_tok = CustomTokenizer(SPTokenizer,model_prefix,pre_rules=default_rules)\n",
    "sp_vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(PATH,'tweet_es_lm_data_SP_more_rules.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ctx_len': 128,\n",
       " 'n_layers': 8,\n",
       " 'n_heads': 12,\n",
       " 'd_model': 512,\n",
       " 'd_head': 64,\n",
       " 'd_inner': 2048,\n",
       " 'resid_p': 0.1,\n",
       " 'attn_p': 0.1,\n",
       " 'ff_p': 0.1,\n",
       " 'embed_p': 0.1,\n",
       " 'output_p': 0.0,\n",
       " 'bias': True,\n",
       " 'scale': True,\n",
       " 'act': <Activation.GeLU: 3>,\n",
       " 'double_drop': False,\n",
       " 'tie_weights': True,\n",
       " 'out_bias': True,\n",
       " 'init': <function fastai.text.models.transformer.init_transformer(m)>,\n",
       " 'mask': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tfmer_lm_config.copy()\n",
    "config['n_layers'] = 8\n",
    "config['ctx_len'] = 128\n",
    "config['d_model'] = 512\n",
    "config['d_inner'] = 2048\n",
    "config['mask'] = True  \n",
    "config['out_bias'] = True\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_language_model(Transformer, len(data.vocab.itos), config=config, drop_mult=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].encoder.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfm_decoder(nn.Module):\n",
    "    def __init__(self,d_model=768,d_vocab=8842, enc:nn.Module=None):\n",
    "        super().__init__()\n",
    "        self.tgt_word_prj = nn.Linear(d_model, d_vocab, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
    "        if enc: self.tgt_word_prj.weight = enc.weight\n",
    "        self.x_logit_scale = (d_model ** -0.5)\n",
    "        \n",
    "    def forward(self,input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs,outputs = input\n",
    "        decoded = self.tgt_word_prj(raw_outputs[0]) * self.x_logit_scale\n",
    "        return decoded, raw_outputs, outputs\n",
    "\n",
    "model[1] = tfm_decoder(512, len(data.vocab.itos), model[0].encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_train import LearnerCallback\n",
    "\n",
    "class my_custom_optimizer(LearnerCallback):\n",
    "    def __init__(self, learn:Learner, d_model, n_warmup_steps):\n",
    "        super().__init__(learn)\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5) ## from Attention paper description\n",
    "        \n",
    "    def on_train_being(self,**kwargs):\n",
    "        self.opt = self.learn.opt\n",
    "\n",
    "    def on_backward_end(self, **kwargs):\n",
    "        'Before .step() and .zero()'\n",
    "        self.n_current_steps += 1\n",
    "        lr_scale =  np.min([\n",
    "                        np.power(self.n_current_steps, -0.5),\n",
    "                        np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "        self.opt.lr = self.init_lr * lr_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): Transformer(\n",
       "    (encoder): Embedding(60000, 512)\n",
       "    (pos_enc): Embedding(128, 512)\n",
       "    (drop_emb): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=2304, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.1)\n",
       "          (drop_res): Dropout(p=0.1)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GeLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (3): Dropout(p=0.1)\n",
       "            (4): MergeLayer()\n",
       "            (5): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): tfm_decoder(\n",
       "    (tgt_word_prj): Linear(in_features=512, out_features=60000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = LanguageLearner(data, model, **{'alpha':0,'beta':0})\n",
    "learn.callbacks.append(my_custom_optimizer(learn,512,4000))\n",
    "learn.unfreeze()\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='25', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/25 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='79' class='' max='5610', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.41% [79/5610 00:23<26:57 10.4246]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(25,callbacks=[SaveModelCallback(learn,every='improvement',mode='max',\n",
    "                                          monitor='accuracy',name='tfm_lm_acc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('twitter_es_enc_Tfm_0516')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(\"twitter_raw_es_more_rules_Tfm_20190516\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 fasta.ai1 DEV",
   "language": "python",
   "name": "fastai1_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
