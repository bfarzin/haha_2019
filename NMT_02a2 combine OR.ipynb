{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMT (seq2seq) in fastai v1\n",
    "\n",
    "Start with this:<br>\n",
    "https://gist.github.com/ohmeow/fe91aed6267cd779946ab9f10eccdab9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, split, build DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_pad_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=False, \n",
    "                        backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    \n",
    "    samples = to_data(samples)\n",
    "    x_max_len = max([len(s[0]) for s in samples])\n",
    "    y_max_len = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    x_res = torch.zeros(len(samples), x_max_len).long() + pad_idx\n",
    "    y_res = torch.zeros(len(samples), y_max_len).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "        \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            x_res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            x_res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,:len(s[1]):] = LongTensor(s[1])\n",
    "            \n",
    "    if backwards: res = res.flip(1)\n",
    "        \n",
    "    return x_res, y_res\n",
    "\n",
    "class Seq2SeqDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, \n",
    "               path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1, pad_first=False, \n",
    "               device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:        \n",
    "        \"\"\"Function that transform the `datasets` in a `DataBunch` for classification.  Passes `**dl_kwargs` on to `DataLoader()`\"\"\"\n",
    "        \n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        \n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)\n",
    "    \n",
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./data/seq2seq/')\n",
    "bs = 64\n",
    "\n",
    "## load the saved data.\n",
    "data = load_data(PATH, \"full_es_en_data_spacyTok.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 255]), torch.Size([64, 252]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(data.train_dl))\n",
    "b[0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][12,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35541, 58838)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), len(data.label_list.train.y.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35541,\n",
       " ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxmaj',\n",
       "  'xxup',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'the',\n",
       "  ',',\n",
       "  '.',\n",
       "  'of',\n",
       "  'to',\n",
       "  'and',\n",
       "  'in',\n",
       "  'that',\n",
       "  'a',\n",
       "  'is',\n",
       "  'we'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), data.label_list.train.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58838,\n",
       " ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxmaj',\n",
       "  'xxup',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'de',\n",
       "  ',',\n",
       "  'la',\n",
       "  '.',\n",
       "  'que',\n",
       "  'en',\n",
       "  'el',\n",
       "  'y',\n",
       "  'a',\n",
       "  'los',\n",
       "  'las'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.y.vocab.itos), data.label_list.train.y.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model for Seq2Seq\n",
    "\n",
    "Big help from this link from the old course + looking at the `Transformer` code and adapting <br>\n",
    "https://github.com/kheyer/ML-DL-Projects/blob/master/Seq2Seq%20Transformer/Transformer.ipynb\n",
    "\n",
    "and here:<br>\n",
    "https://nbviewer.jupyter.org/github/fastai/fastai/blob/6ba17b21599a6fc441794ffd130bc31b5333b4a0/courses/dl2/translate.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., dropout:float=0.2, bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.att_q = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_k = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_v = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(dropout),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None, **kwargs):\n",
    "        \"attn -> Linear -> drop -> merge -> LN\"\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = q.size(0),q.size(1) # bs x bptt x d_model\n",
    "        wq,wk,wv = self.att_q(q), self.att_k(k), self.att_v(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None: \n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish}\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "    \n",
    "class EncDecLayer(nn.Module):\n",
    "    \"Decoder block for seq2seq. Self and target attention combined.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0.2, ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=MultiHeadAttention, is_decode=False):\n",
    "        super().__init__()\n",
    "        self.is_decode = is_decode\n",
    "        self.mhra_s    = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, dropout=attn_p, bias=bias, scale=scale)\n",
    "        if self.is_decode:\n",
    "            self.mhra_targ = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, dropout=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "        \n",
    "    def forward(self, x:Tensor, enc_out:Tensor=None, src_mask:Tensor=None, trg_mask:Tensor=None, **kwargs):\n",
    "        assert self.is_decode == (enc_out is not None), \"Calling Decode `forward()` with out init `is_decode`\"\n",
    "        ## I think I had the wrong masks here, had them flipped around.\n",
    "        x = self.mhra_s(x,x,x, mask=trg_mask if self.is_decode else src_mask, **kwargs)\n",
    "        if self.is_decode: x = self.mhra_targ(x, enc_out, enc_out, mask=src_mask, **kwargs)\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, is_decode=False):\n",
    "        super().__init__()\n",
    "        self.is_decode = is_decode\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, 255)\n",
    "        self.layers = nn.ModuleList([EncDecLayer(heads, d_model, d_head=d_model//heads, d_inner=2048, is_decode=is_decode) \n",
    "                                         for k in range(N)])\n",
    "    \n",
    "    def forward(self, x, e_outputs=None, src_mask=None, trg_mask=None):\n",
    "        x = self.pe(self.embed(x))\n",
    "        if (not self.is_decode) and (e_outputs is None):\n",
    "            for layer in self.layers:  x = layer(x,src_mask=src_mask)\n",
    "        elif self.is_decode:\n",
    "            for layer in self.layers:  x = layer(x, e_outputs, src_mask, trg_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = EncDec(src_vocab, d_model, N, heads)\n",
    "        self.decoder = EncDec(trg_vocab, d_model, N, heads, is_decode=True)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "        \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
    "        if src_mask is None: src_mask, trg_mask = create_masks(src, trg)\n",
    "        e_outputs = self.encoder(src, src_mask=src_mask)\n",
    "        d_output  = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return [output, output, output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 5\n",
    "torch.tril(torch.ones((size,size)), diagonal=0).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones((size,size)), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size):\n",
    "    \"valid view locations lower trianglular including diagonal\"\n",
    "    return torch.triu(torch.ones((size,size)), diagonal=1).byte().unsqueeze(0)\n",
    "\n",
    "def create_masks(src, trg=None):\n",
    "    \"masks for nopeak and remove padding from training\"\n",
    "    src_mask = (src == 1).unsqueeze(-2)\n",
    "    if trg is not None: trg_mask = (trg == 1).unsqueeze(-2) | nopeak_mask(trg.size(1)).cuda() #change to or?\n",
    "    else: trg_mask = None\n",
    "\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 150\n",
    "heads = 3\n",
    "N = 3\n",
    "\n",
    "src_vocab = len(data.label_list.train.x.vocab.itos)\n",
    "trg_vocab = len(data.label_list.train.y.vocab.itos)\n",
    "model = to_device(Transformer(src_vocab, trg_vocab, d_model, N, heads), defaults.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): EncDec(\n",
       "    (embed): Embedding(35541, 150)\n",
       "    (pe): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): EncDec(\n",
       "    (embed): Embedding(58838, 150)\n",
       "    (pe): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=150, out_features=58838, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AppendBatchTargs(Callback):\n",
    "    \"Include the target in the training loop for Decoder mask\"\n",
    "    learn:Learner\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        return {'last_input':(last_input, last_target[:,:-1]),\n",
    "                'last_target':last_target[:,1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLearner(RNNLearner):\n",
    "    \"Subclass of RNNLearner for predictions using Seq2Seq\"\n",
    "    \n",
    "    def predict(self, text:str, n_words:int=1, no_unk:bool=True, temperature:float=1., min_p:float=None, sep:str=' ',\n",
    "                decoder=decode_spec_tokens):\n",
    "        \"Return the `n_words` that come after `text`.\"\n",
    "        ## handle predictions for Seq2Seq\n",
    "        set_trace()\n",
    "        ds = self.data.single_dl.dataset\n",
    "        self.model.reset()\n",
    "        xb,yb = self.data.one_item(text)\n",
    "        new_idx = []\n",
    "        for _ in range(n_words): #progress_bar(range(n_words), leave=False):\n",
    "            res = self.pred_batch(batch=(xb,yb))[0][-1]\n",
    "            #if len(new_idx) == 0: self.model[0].select_hidden([0])\n",
    "            if no_unk: res[self.data.vocab.stoi[UNK]] = 0.\n",
    "            if min_p is not None: res[res < min_p] = 0.\n",
    "            if temperature != 1.: res.pow_(1 / temperature)\n",
    "            idx = torch.multinomial(res, 1).item()\n",
    "            new_idx.append(idx)\n",
    "            xb = xb.new_tensor([idx])[None]\n",
    "        return text + sep + sep.join(decoder(self.data.vocab.textify(new_idx, sep=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Seq2SeqLearner(data, model, **{'alpha':0,'beta':0}, callbacks=[AppendBatchTargs()], loss_func=CrossEntropyFlat())\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27,560,876'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in learn.model.parameters() if p.requires_grad)\n",
    "f'{total_params:,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in learn.model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.batch_size = 24  ## 64 fails to load.  Prob. too big embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31011' class='' max='33232', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      93.32% [31011/33232 34:17<02:27 1.2405]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-3)  #flipping the mask seemed to have caused a problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.valid_dl))\n",
    "x,y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.model(x,y)\n",
    "preds[0][0,:].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_itos = data.label_list.train.x.vocab.itos\n",
    "y_itos = data.label_list.train.y.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(5):\n",
    "    print(' '.join([x_itos[o] for o in x[i,:] if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in y[i,:] if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in preds[0][i,:].argmax(dim=1) if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        try: out = model.decoder(Variable(ys), memory, src_mask, Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        except: set_trace()\n",
    "        prob = F.softmax(model.out(out[:, -1]))\n",
    "\n",
    "#         next_word = torch.multinomial(prob, 1)  #sample from distribution\n",
    "#         next_word = next_word.data[0][0]\n",
    "\n",
    "        next_word = prob.argmax(dim=-1).item()  ## single best\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = x\n",
    "trg = y\n",
    "trg_input = trg[:,:-1]\n",
    "# targets = trg[:, 1:].contiguous()\n",
    "src_mask, _ = create_masks(src, trg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = greedy_decode(m, src[10].unsqueeze(0), src_mask[10], max_len=y.size(1), start_symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([x_itos[o] for o in src[10] if o != 1]))\n",
    "print(' '.join([y_itos[o] for o in trg[10] if o != 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([y_itos[o] for o in ys[0,:] if o != 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 fasta.ai1 DEV",
   "language": "python",
   "name": "fastai1_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
