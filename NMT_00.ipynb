{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMT (seq2seq) in fastai v1\n",
    "\n",
    "Start with this:<br>\n",
    "https://gist.github.com/ohmeow/fe91aed6267cd779946ab9f10eccdab9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farzin/haha_2019\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_es_translate.csv  es-en.tgz\teuroparl-v7.es-en.en  europarl-v7.es-en.es\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data/es-en')\n",
    "\n",
    "BASE = 'europarl-v7.es-en'\n",
    "en_file = DATA_PATH/f'{BASE}.en'\n",
    "es_file = DATA_PATH/f'{BASE}.es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_sq = re.compile('^([^?.!]+\\?)')\n",
    "\n",
    "lines = ((re_eq.search(eq), re_sq.search(sq)) \n",
    "          for eq, sq in zip(open(en_file, encoding='utf-8'), open(es_file, encoding='utf-8')))\n",
    "\n",
    "qs = [ {'english_text': e.group(), 'spanish_text': f.group()} for e, f in lines if e and f ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'english_text': 'Why has no air quality test been done on this particular building since we were elected?',\n",
       "  'spanish_text': '¿Por qué no se ha hecho ninguna prueba de calidad del aire de este edificio desde que hemos sido elegidos?'},\n",
       " {'english_text': 'Why has there been no Health and Safety Committee meeting since 1998?',\n",
       "  'spanish_text': '¿Por qué no se ha celebrado ninguna reunión del Comité de Sanidad y Seguridad desde 1998?'},\n",
       " {'english_text': 'Why has there been no fire drill, either in the Brussels Parliament buildings or the Strasbourg Parliament buildings?',\n",
       "  'spanish_text': '¿Por qué no hemos tenido simulacros de incendio ni en los edificios del Parlamento de Bruselas ni en los del Parlamento de Estrasburgo?'},\n",
       " {'english_text': 'Why are there no fire instructions?',\n",
       "  'spanish_text': '¿Por qué no hay instrucciones en caso de incendio?'},\n",
       " {'english_text': 'Why have the staircases not been improved since my accident?',\n",
       "  'spanish_text': '¿Por qué no se han mejorado las escaleras desde mi accidente?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_text</th>\n",
       "      <th>spanish_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why has no air quality test been done on this ...</td>\n",
       "      <td>¿Por qué no se ha hecho ninguna prueba de cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why has there been no Health and Safety Commit...</td>\n",
       "      <td>¿Por qué no se ha celebrado ninguna reunión de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why has there been no fire drill, either in th...</td>\n",
       "      <td>¿Por qué no hemos tenido simulacros de incendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why are there no fire instructions?</td>\n",
       "      <td>¿Por qué no hay instrucciones en caso de incen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why have the staircases not been improved sinc...</td>\n",
       "      <td>¿Por qué no se han mejorado las escaleras desd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_text  \\\n",
       "0  Why has no air quality test been done on this ...   \n",
       "1  Why has there been no Health and Safety Commit...   \n",
       "2  Why has there been no fire drill, either in th...   \n",
       "3                Why are there no fire instructions?   \n",
       "4  Why have the staircases not been improved sinc...   \n",
       "\n",
       "                                        spanish_text  \n",
       "0  ¿Por qué no se ha hecho ninguna prueba de cali...  \n",
       "1  ¿Por qué no se ha celebrado ninguna reunión de...  \n",
       "2  ¿Por qué no hemos tenido simulacros de incendi...  \n",
       "3  ¿Por qué no hay instrucciones en caso de incen...  \n",
       "4  ¿Por qué no se han mejorado las escaleras desd...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(qs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH/'en_es_translate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, split, build DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11813, 2), (2953, 2))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH/'en_es_translate.csv')\n",
    "\n",
    "rnd_seed = 20190313\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "idx = np.random.permutation(len(df))\n",
    "valid_cut = int(0.20 * len(idx))\n",
    "\n",
    "train_df = df.iloc[idx[:-valid_cut],:]\n",
    "valid_df = df.iloc[idx[-valid_cut:],:]\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_pad_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=False, \n",
    "                        backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    \n",
    "    samples = to_data(samples)\n",
    "    x_max_len = max([len(s[0]) for s in samples])\n",
    "    y_max_len = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    x_res = torch.zeros(len(samples), x_max_len).long() + pad_idx\n",
    "    y_res = torch.zeros(len(samples), y_max_len).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "        \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            x_res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            x_res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,:len(s[1]):] = LongTensor(s[1])\n",
    "            \n",
    "    if backwards: res = res.flip(1)\n",
    "        \n",
    "    return x_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, \n",
    "               path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1, pad_first=False, \n",
    "               device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:        \n",
    "        \"\"\"Function that transform the `datasets` in a `DataBunch` for classification.  Passes `**dl_kwargs` on to `DataLoader()`\"\"\"\n",
    "        \n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        \n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = Tokenizer(lang='en')\n",
    "es_tok = Tokenizer(lang='es')\n",
    "\n",
    "en_procs = [TokenizeProcessor(tokenizer=en_tok), NumericalizeProcessor()]\n",
    "es_procs = [TokenizeProcessor(tokenizer=es_tok), NumericalizeProcessor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./data/seq2seq/')\n",
    "bs = 64\n",
    "\n",
    "en_train_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['english_text'], processor=en_procs)\n",
    "es_train_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['spanish_text'], processor=es_procs)\n",
    "\n",
    "en_valid_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['english_text'])\n",
    "es_valid_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['spanish_text'])\n",
    "\n",
    "trn_ll = LabelList(en_train_il, es_train_il)\n",
    "val_ll = LabelList(en_valid_il, es_valid_il)\n",
    "\n",
    "lls = LabelLists(PATH, train=trn_ll, valid=val_ll).process()\n",
    "\n",
    "data = lls.databunch(bs=bs, val_bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052, 4658)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_train_il.vocab.itos), len(es_train_il.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 155]), torch.Size([64, 147]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(data.train_dl))\n",
    "b[0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    5,   11,  ...,    5, 1338,   10],\n",
       "        [   2,    5,   11,  ...,    1,    1,    1],\n",
       "        [   2,    5,   36,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2,    5,   36,  ...,    1,    1,    1],\n",
       "        [   2,    5,   36,  ...,    1,    1,    1],\n",
       "        [   2,    5,   18,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   10,    5,  ..., 1791, 3303,    9],\n",
       "        [   2,   10,    5,  ...,    1,    1,    1],\n",
       "        [   2,    5,   56,  ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2,   10,    5,  ...,    1,    1,    1],\n",
       "        [   2,    5,   56,  ...,    1,    1,    1],\n",
       "        [   2,   10,    5,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052, 4658)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), len(data.label_list.train.y.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052,\n",
       " ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxmaj',\n",
       "  'xxup',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'the',\n",
       "  '?',\n",
       "  'what',\n",
       "  'to',\n",
       "  ',',\n",
       "  'of',\n",
       "  'is',\n",
       "  'in',\n",
       "  'and',\n",
       "  'why',\n",
       "  'we'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), data.label_list.train.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Seq2SeqDataBunch"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model for Seq2Seq\n",
    "\n",
    "Big help from this link from the old course + looking at the `Transformer` code and adapting <br>\n",
    "https://github.com/kheyer/ML-DL-Projects/blob/master/Seq2Seq%20Transformer/Transformer.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m_MultiHeadAttention(nn.Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.att_q = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_k = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_v = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None, **kwargs):\n",
    "        \"attn -> Linear -> drop -> merge -> LN\"\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = q.size(0),q.size(1) # bs x bptt x d_model\n",
    "        wq,wk,wv = self.att_q(q), self.att_k(k), self.att_v(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish}\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "class m_EncoderLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=m_MultiHeadAttention):\n",
    "        super().__init__()\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, x, x, mask=mask, **kwargs))\n",
    "\n",
    "class m_DecoderLayer(nn.Module):\n",
    "    \"Decoder block for seq2seq. Self and target attention combined.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=m_MultiHeadAttention):\n",
    "        super().__init__()\n",
    "        self.mhra_s    = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.mhra_targ = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "        \n",
    "    def forward(self, x:Tensor, enc_out:Tensor, src_mask:Tensor=None, targ_mask:Tensor=None, **kwargs): \n",
    "        x = self.mhra_s(x,x,x, mask=targ_mask, **kwargs)\n",
    "        return self.ff(self.mhra_targ(x, enc_out, enc_out, mask=src_mask, **kwargs))\n",
    "    \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, tgt_vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=m_MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([m_EncoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                          ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                          attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "        self.decoder = nn.Embedding(tgt_vocab_sz, d_model)\n",
    "        self.pos_dec = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_dec = nn.Dropout(embed_p)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([m_DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                          ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                          attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        self.tgt_word_prj = nn.Linear(d_model, tgt_vocab_sz, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
    "        self.x_logit_scale = (d_model ** -0.5)\n",
    "        \n",
    "    def reset(self):pass\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        bs, x_len = x.size()\n",
    "        bs, y_len = y.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype)\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "        pos_y = torch.arange(0, y_len, device=x.device, dtype=x.dtype)\n",
    "        targ = self.drop_dec(self.decoder(y) + self.pos_dec(pos_y)[None]) #.mul_(self.d_model ** 0.5)\n",
    "\n",
    "        ## masking/padding is not yet right here.  Needs to be fixed to mask the pad IDs\n",
    "        src_mask = (x==1).byte()[:,None,None,:] #[64,5,155,155]\n",
    "        #mask == trg_mask (but trg_mask also masks out all xxpad ids [id==1]  add that here)\n",
    "        nopeak_mask = torch.triu(x.new_ones(y_len, y_len), diagonal=1).byte() if self.mask else None\n",
    "        targ_mask = (y==1).byte()[:,None,:,None] * nopeak_mask\n",
    "        \n",
    "        for layer in self.enc_layers: inp  = layer(inp, mask=src_mask)\n",
    "        for layer in self.dec_layers: targ = layer(targ, inp, src_mask=src_mask, targ_mask=targ_mask)\n",
    "        decoded = self.tgt_word_prj(targ) * self.x_logit_scale\n",
    "\n",
    "        return [decoded,decoded,decoded] #for RNN trainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfm_seq2seq = Seq2SeqTransformer(vocab_sz=len(data.label_list.train.x.vocab.itos),\n",
    "                       tgt_vocab_sz=len(data.label_list.train.y.vocab.itos),\n",
    "                       ctx_len=256, n_layers=6, n_heads=10, d_model=300, d_head=64, d_inner=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_seq2seq = Seq2SeqTransformer(vocab_sz=len(data.label_list.train.x.vocab.itos),\n",
    "                       tgt_vocab_sz=len(data.label_list.train.y.vocab.itos),\n",
    "                       ctx_len=256, n_layers=6, n_heads=5, d_model=768, d_head=64, d_inner=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 0., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 1., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tfm_seq2seq.apply(init_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I add `on_batch_begin` to return x,y as `xb` then that is passed as `*xb` to the model and we can extract y and use for teacher forcing and proper attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AppendBatchTargs(Callback):\n",
    "    learn:Learner\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        return {'last_input':(last_input, last_target), 'last_target':last_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, tfm_seq2seq, **{'alpha':0,'beta':0}, callbacks=[AppendBatchTargs()], loss_func=CrossEntropyFlat())\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'47,327,616'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in learn.model.parameters() if p.requires_grad)\n",
    "f'{total_params:,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='25', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [5/25 02:57<11:50]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>6.225850</th>\n",
       "    <th>5.730264</th>\n",
       "    <th>00:35</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.681012</th>\n",
       "    <th>3.360382</th>\n",
       "    <th>00:36</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>2.101718</th>\n",
       "    <th>2.061979</th>\n",
       "    <th>00:35</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.328639</th>\n",
       "    <th>1.381029</th>\n",
       "    <th>00:36</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.924974</th>\n",
       "    <th>0.974917</th>\n",
       "    <th>00:34</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='39' class='' max='184', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      21.20% [39/184 00:06<00:25 0.8539]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fit one cycle is just too darn agressive for Transformer. Why?\n",
    "learn.fit(25, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(learn.data.train_dl))\n",
    "\n",
    "src = x[0,:]\n",
    "targ = y[0,:]\n",
    "\n",
    "preds = learn.model(src.unsqueeze(0), targ.unsqueeze(0))\n",
    "preds[0].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 5, 5, 5, 5, 5, 9, 9, 5, 5, 9, 5, 9, 5, 5, 9, 5, 5, 1, 5, 5, 5, 5,\n",
       "         9, 9, 9, 9, 9, 5, 9, 5, 9, 5, 9, 9, 5, 5, 5, 5, 9, 1, 9, 9, 1, 5, 5, 1,\n",
       "         5, 5, 9, 5, 5, 9, 9, 5, 5, 5, 5, 5, 9, 9, 9, 9, 1, 5, 9, 5, 5, 1, 9, 5,\n",
       "         1, 9, 5, 5, 5, 5, 9, 9, 9, 5, 5, 5, 5, 5, 5, 5, 9, 5, 9, 9, 9, 5, 1, 5,\n",
       "         5, 9, 5, 9, 5, 5, 5, 5, 5, 9, 1, 1, 5, 5, 1, 5, 9, 9, 9, 9, 5, 5, 9, 5,\n",
       "         5, 1, 5, 5, 5, 9, 5, 9, 9, 1, 5, 9, 5, 5, 5, 5, 9, 9, 5, 5, 9, 9, 1, 1,\n",
       "         9, 9, 5]], device='cuda:0')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  ## all predicting padding! haha.  something not right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxpad'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label_list.train.x.vocab.itos[1] ## must be because there are a lot of padding vals in there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Embedding(4052, 300)\n",
       "  (pos_enc): Embedding(256, 300)\n",
       "  (drop_emb): Dropout(p=0.0)\n",
       "  (enc_layers): ModuleList(\n",
       "    (0): m_EncoderLayer(\n",
       "      (mhra): m_MultiHeadAttention(\n",
       "        (att_q): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_k): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_v): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (out): Linear(in_features=320, out_features=300, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=300, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.0)\n",
       "          (3): Linear(in_features=128, out_features=300, bias=True)\n",
       "          (4): Dropout(p=0.0)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Embedding(4658, 300)\n",
       "  (pos_dec): Embedding(256, 300)\n",
       "  (drop_dec): Dropout(p=0.0)\n",
       "  (dec_layers): ModuleList(\n",
       "    (0): m_DecoderLayer(\n",
       "      (mhra_s): m_MultiHeadAttention(\n",
       "        (att_q): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_k): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_v): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (out): Linear(in_features=320, out_features=300, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (mhra_targ): m_MultiHeadAttention(\n",
       "        (att_q): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_k): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (att_v): Linear(in_features=300, out_features=320, bias=True)\n",
       "        (out): Linear(in_features=320, out_features=300, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=300, out_features=128, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.0)\n",
       "          (3): Linear(in_features=128, out_features=300, bias=True)\n",
       "          (4): Dropout(p=0.0)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([300]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tgt_word_prj): Linear(in_features=300, out_features=4658, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 fasta.ai1 DEV",
   "language": "python",
   "name": "fastai1_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
