%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Insert your title here%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{Short form of title}        % if too long for running head

\author {\textbf{Bobak Farzin$^1$}, \textbf{Nombre Apellidos2$^2$}\\
	$^1$bfarzin@gmail.com\\
	$^2$Universidad o lugar de trabajo\\
}

%\author{Bobak Farzin         \and
%        Second Author %etc.
%}

\authorrunning{Short form of author list} % if too long for running head

%\institute{F. Author \at
%              first address \\
%              Tel.: +123-45-678910\\
%              Fax: +123-45-678910\\
%              \email{bfarzin@gmail.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
%           \and
%           S. Author \at
%              second address
%}

\date{Received: 15 June 2019 / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

All the code for this project is included in a GitHub \cite{} repository for easy reference.  Some details are omitted below for clarity but can be found in the notebooks, code and execution instructions.

\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}
Included citations as necessary: ~\cite{Nobody06}
\paragraph{Contribution} Our contribution with this work is..

\section{Task and Dataset Description}
\label{sec:task}
The \textit{Humor Analysis based on Humor Annotation (HAHA)} competition asked for analysis of two tasks in the Spanish language based on a corpus of publicly collected data ~\cite{castro2018crowd}:
\begin{itemize}
\item \textbf{Task1: Humor Detection}:Detemine if a tweet is humorous. System ranking is based on F1 score which will balance precision and accuracy.
\item \textbf{Task2: Funniness Score}:If humorous, what is the average humor rating of the tweet. System ranking is based on RMSE.
\end{itemize}
The HAHA dataset includes labeled data for 24,000 tweets and a test set of 6,000 tweets (80\%/20\% train/test split.)  Each record includes the raw tweet text (including accents and emoticons), a binary humor label, the number of votes for each of five star ratings and a ``Funninness Score'' that is the average of the 1 to 5 star votes cast.  Examples and data can be found on the CodaLab competition webpage\footnote{http://competitions.codalab.org/competitions/22194/}.

\section{System Description}
\label{sec:system}
We generally follow the method of ULMFiT ~\cite{HowardRuder:DBLP:journals/corr/abs-1801-06146} including pre-training and differential learning rates. 
\begin{enumerate}
	\item Train a language model (LM) on a large corpus of data
	\item Fine-tune the LM based on the target task language data
	\item Replace the final layer of the LM with a softmax or linear output layer and then fine-tune on the particular task at hand (classification or regression)
\end{enumerate}
Below we will give more detail on each step and the parameters used to generate our system.
\subsection{Data, Cleanup \& Tokenization}
\label{sec:datacleaning}
\subsection{Additional Data}
For our initial training, we  collected 475,143 tweets in the Spanish language using tweepy ~\cite{Tweepy}.  The frequency of terms, punctuation and vocabulary can be quite different from the standard Wikipedia corpus that often used to train an LM.

For the fine-tuning step, we combined the labeled and un-labeled text data for the LM training.
\subsection{Cleaning}
We applied a list of default cleanup functions included in Fastai and added an additional one for this Twitter dataset.
\begin{itemize}
	\item Add spaces between special chars (ie. \verb|!!!| to \verb|! ! !|)
	\item Remove useless spaces (remove more than 2 spaces in sequence)
	\item Replace repetition at the character level (ie. \verb|grrrreat| becomes \verb|g xxrep r 3 eat|)
	\item Replace repition at the word level (similar to above)
	\item Deal with ALL CAPS words replacing with a token and converting to lower case.
	\item \textbf{Addition:} Move all text onto a single line by replacing new-lines inside a tweet with a reserved word (ie. \verb|\n| to \verb|xxnl|)
\end{itemize} 
%\pagebreak  %move as needed to keep tweet text together
The following example shows the application of this data cleaning to a single tweet:
\begin{verbatim} 
Saber, entender y estar convencides que la frase \
#LaESILaDefendemosEntreTodes es nuestra linea es nuestro eje.\
#AlertaESI!!!!
Vamos por mas!!! e invitamos a todas aquellas personas que quieran \
se parte.
\end{verbatim}

\begin{verbatim} 
xxbos saber , entender y estar convencides que la frase \
# laesiladefendemosentretodes es nuestra linea es nuestro eje.\
xxnl  # alertaesi xxrep 4 ! xxnl vamos por mas ! ! ! e invitamos a \
todas aquellas personas que quieran se parte.
\end{verbatim}

\subsection{Tokenization}
We used sentencepiece~\cite{SentencePiece:DBLP:journals/corr/abs-1808-06226} to parse into sub-word units and reduce the possible out-of-vocabulary (OOV) terms in the data set.  We selected a vocab size of 30,000 and used the byte-pair encoding (bpe) model. 

\section{Training and Results}
\label{sec:4}
\subsection{LM Training and Fine-tuning}
We train the LM using a 90/10 training/validation split and report the validation loss and accuracy of next-word prediction on the validation set.  For the LM, we selected a AWD\_LSTM \cite{merity?} model included in Fastai with QRNN units, 2304 hidden-states, 3 layers and a softmax layer on the end to predict the next-word.  We tied the embedding weights on the encoder and decoder for training.  We performed some simple tests with LSTM units and with a Transformer Language model and found all the models were similar in performance during LM training.  We chose the QRNN units to speed up training. The model has about 60 million trainable parameters.  

Our loss is label smoothing\cite{labelsmooth} of the flattened cross-entropy loss. 
Parameters used for training and finetuning are shown in Table \ref{tab:tab_training}.
For all the networks we applied a dropout multiplier that scales the dropout used throughout the network.  We used the Adam optimizer with weight decay as indicated in the table.  

Following the work of \cite{smith}  we found the largest learning-rate that we could apply and then ran a one-cycle policy \cite{} for a single epoch. We then ran subsequent training with lower learning rates, again indicated in Table \ref{tab:tab_training}.

\begin{table}[ht]
	% table caption is above the table
	\caption{LM Training Parameters}
	\label{tab:tab_training}       % Give a unique label
\begin{tabular}{lll}
	\hline\noalign{\smallskip}
	Param & LM & Fine-Tune LM \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	Weight Decay & 0.1 & 0.1 \\
	Dropout Mult & 1.0 & 1.0 \\
	Learning Rate & 1 epoch at $5*10^{-3}$ & 5 epochs at $3*10^{-3}$ \\
    Cont. Training & 15 epochs at $1*10^{-3}$ & 10 epochs at $1*10^{-4}$\\
	\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\subsection{Classification and Regression Fitting}
Again, following the playbook from \cite{HowardRuder:DBLP:journals/corr/abs-1801-06146}, we change out the head of the network to a softmax or linear output layer and then load the LM weights for the layers below.  We train just the new head from the random init, then unfreeze the entire network and train with differential learning rats.  With the classifier, we again used label smoothing which allows us to train without gradual unfreezing.  

With the same learning rate and weight decay we apply a 5-fold cross-validation on the outputs and take the mean across the folds as our ensemble.  We sample 20 random seeds (see more in sec \ref{sec:rand_seeds}) to find the best initialization for our gradient descent search.  From these samples, we select the best validation F1 metric or MSE for use in our test submission.
\subsubsection{Classifier setup}  For the classifier, we have a softmax head and label smoothing loss.  We oversample the minority class balance the outcomes for better training using SMOTE \cite{Chawla:2002:SSM:1622407.1622416}.  
\subsubsection{Regression setup}  For the regression, we fill all N/A labels with scores of 0.  We add a linear head output and mean-squared-error (MSE) loss function. 

\begin{table}[ht]
	% table caption is above the table
	\caption{Classificaiton and Regression Training Parameters}
	\label{tab:clas_training}       % Give a unique label
	\begin{tabular}{ll}
		\hline\noalign{\smallskip}
		Param & Value \\
		\noalign{\smallskip}\hline\noalign{\smallskip}
		Weight Decay & 0.1  \\
		Dropout Mult &  0.7 \\
		Learning Rate (Head)& 2 epochs at $1*10^{-2}$\\
		Cont. Training & 15 epochs with diff lr:($1*10^{-3}/(2.6^4)$, $5*10^{-3}$)\\
		\noalign{\smallskip}\hline
	\end{tabular}
\end{table}

\subsection{Random Seed as a Hyperparamter}
\label{sec:rand_seeds}
For classification and regression, the random seed sets the initial random weights of the head layer. This initialization effects the final F1 metric you can achieve with your model.  

Across each of the 20 random seeds, we average the 5-folds and get a single F1 metric on the validation set.  The histogram of outcomes is shown in Figure \ref{fig:random_seed_hist} and covers a range  from 0.820 to 0.825 in the validation set. We selected our single best random seed for the test submission. With more exploration, a better seed could likely be found.  We only tried a single seed for the LM training but one could do a similar search with random seeds there and select the best down-stream seed similar to \cite{poleval}

\begin{figure}[h]
	% Use the relevant command to insert your figure file.
	% For example, with the graphicx package use
	\includegraphics[width=0.75\textwidth]{seed_hist_f1}
	\caption{Histogram of F1 metric averaged across 5-fold metric}
	\label{fig:random_seed_hist}
\end{figure}


\section{Conclusion}
\label{sec:5}


\begin{acknowledgements}
If you'd like to thank anyone, place your comments here
and remove the percent signs.
\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{local}   % name your BibTeX data base

\end{document}
% end of file template.tex

