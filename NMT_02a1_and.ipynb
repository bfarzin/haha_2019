{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMT (seq2seq) in fastai v1\n",
    "\n",
    "Start with this:<br>\n",
    "https://gist.github.com/ohmeow/fe91aed6267cd779946ab9f10eccdab9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, split, build DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_pad_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=False, \n",
    "                        backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    \n",
    "    samples = to_data(samples)\n",
    "    x_max_len = max([len(s[0]) for s in samples])\n",
    "    y_max_len = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    x_res = torch.zeros(len(samples), x_max_len).long() + pad_idx\n",
    "    y_res = torch.zeros(len(samples), y_max_len).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "        \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            x_res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            x_res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,:len(s[1]):] = LongTensor(s[1])\n",
    "            \n",
    "    if backwards: res = res.flip(1)\n",
    "        \n",
    "    return x_res, y_res\n",
    "\n",
    "class Seq2SeqDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, \n",
    "               path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1, pad_first=False, \n",
    "               device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:        \n",
    "        \"\"\"Function that transform the `datasets` in a `DataBunch` for classification.  Passes `**dl_kwargs` on to `DataLoader()`\"\"\"\n",
    "        \n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        \n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)\n",
    "    \n",
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('./data/seq2seq/')\n",
    "bs = 64\n",
    "\n",
    "## load the saved data.\n",
    "data = load_data(PATH, \"full_es_en_data_spacyTok.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 255]), torch.Size([64, 252]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(data.train_dl))\n",
    "b[0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][12,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35541, 58838)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), len(data.label_list.train.y.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35541,\n",
       " ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxmaj',\n",
       "  'xxup',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'the',\n",
       "  ',',\n",
       "  '.',\n",
       "  'of',\n",
       "  'to',\n",
       "  'and',\n",
       "  'in',\n",
       "  'that',\n",
       "  'a',\n",
       "  'is',\n",
       "  'we'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.x.vocab.itos), data.label_list.train.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58838,\n",
       " ['xxunk',\n",
       "  'xxpad',\n",
       "  'xxbos',\n",
       "  'xxeos',\n",
       "  'xxfld',\n",
       "  'xxmaj',\n",
       "  'xxup',\n",
       "  'xxrep',\n",
       "  'xxwrep',\n",
       "  'de',\n",
       "  ',',\n",
       "  'la',\n",
       "  '.',\n",
       "  'que',\n",
       "  'en',\n",
       "  'el',\n",
       "  'y',\n",
       "  'a',\n",
       "  'los',\n",
       "  'las'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.label_list.train.y.vocab.itos), data.label_list.train.y.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model for Seq2Seq\n",
    "\n",
    "Big help from this link from the old course + looking at the `Transformer` code and adapting <br>\n",
    "https://github.com/kheyer/ML-DL-Projects/blob/master/Seq2Seq%20Transformer/Transformer.ipynb\n",
    "\n",
    "and here:<br>\n",
    "https://nbviewer.jupyter.org/github/fastai/fastai/blob/6ba17b21599a6fc441794ffd130bc31b5333b4a0/courses/dl2/translate.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this should be directly from the transformer.py file\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish}\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., dropout:float=0.2, bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.att_q = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_k = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_v = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(dropout),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None, **kwargs):\n",
    "        \"attn -> Linear -> drop -> merge -> LN\"\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = q.size(0),q.size(1) # bs x bptt x d_model\n",
    "        wq,wk,wv = self.att_q(q), self.att_k(k), self.att_v(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None: \n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn_score = attn_score.float().masked_fill(mask == 0, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class EncDecLayer(nn.Module):\n",
    "    \"Decoder block for seq2seq. Self and target attention combined.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0.2, ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=MultiHeadAttention, is_decode=False):\n",
    "        super().__init__()\n",
    "        self.is_decode = is_decode\n",
    "        self.mhra_s    = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, dropout=attn_p, bias=bias, scale=scale)\n",
    "        if self.is_decode:\n",
    "            self.mhra_targ = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, dropout=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "        \n",
    "    def forward(self, x:Tensor, enc_out:Tensor=None, src_mask:Tensor=None, trg_mask:Tensor=None, **kwargs):\n",
    "        assert self.is_decode == (enc_out is not None), \"Calling Decode `forward()` with out init `is_decode`\"\n",
    "        ## I think I had the wrong masks here, had them flipped around.\n",
    "        x = self.mhra_s(x,x,x, mask=trg_mask if self.is_decode else src_mask, **kwargs)\n",
    "        if self.is_decode: x = self.mhra_targ(x, enc_out, enc_out, mask=src_mask, **kwargs)\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDec(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, is_decode=False):\n",
    "        super().__init__()\n",
    "        self.is_decode = is_decode\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, 255)\n",
    "        self.layers = nn.ModuleList([EncDecLayer(heads, d_model, d_head=d_model//heads, d_inner=2048, is_decode=is_decode) \n",
    "                                         for k in range(N)])\n",
    "    \n",
    "    def forward(self, x, e_outputs=None, src_mask=None, trg_mask=None):\n",
    "        x = self.pe(self.embed(x))\n",
    "        if (not self.is_decode) and (e_outputs is None):\n",
    "            for layer in self.layers:  x = layer(x,src_mask=src_mask)\n",
    "        elif self.is_decode:\n",
    "            for layer in self.layers:  x = layer(x, e_outputs, src_mask, trg_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = EncDec(src_vocab, d_model, N, heads)\n",
    "        self.decoder = EncDec(trg_vocab, d_model, N, heads, is_decode=True)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "        \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
    "        if src_mask is None: src_mask, trg_mask = create_masks(src, trg)\n",
    "        e_outputs = self.encoder(src, src_mask=src_mask)\n",
    "        d_output  = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return [output, output, output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size):\n",
    "    \"valid view locations lower trianglular including diagonal\"\n",
    "    return torch.tril(torch.ones((size,size)), diagonal=0).byte().unsqueeze(0)\n",
    "\n",
    "def create_masks(src, trg=None):\n",
    "    \"masks for nopeak and remove padding from training\"\n",
    "    src_mask = (src != 1).unsqueeze(-2)\n",
    "    if trg is not None: trg_mask = (trg != 1).unsqueeze(-2) & nopeak_mask(trg.size(1)).cuda()\n",
    "    else: trg_mask = None\n",
    "\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 150\n",
    "heads = 3\n",
    "N = 3\n",
    "\n",
    "src_vocab = len(data.label_list.train.x.vocab.itos)\n",
    "trg_vocab = len(data.label_list.train.y.vocab.itos)\n",
    "model = to_device(Transformer(src_vocab, trg_vocab, d_model, N, heads), defaults.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): EncDec(\n",
       "    (embed): Embedding(35541, 150)\n",
       "    (pe): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): EncDec(\n",
       "    (embed): Embedding(58838, 150)\n",
       "    (pe): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncDecLayer(\n",
       "        (mhra_s): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (mhra_targ): MultiHeadAttention(\n",
       "          (att_q): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_k): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (att_v): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (out): Linear(in_features=150, out_features=150, bias=True)\n",
       "          (drop_att): Dropout(p=0.2)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=150, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=150, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([150]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=150, out_features=58838, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AppendBatchTargs(Callback):\n",
    "    \"Include the target in the training loop for Decoder mask\"\n",
    "    learn:Learner\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        return {'last_input':(last_input, last_target[:,:-1]),\n",
    "                'last_target':last_target[:,1:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqLearner(RNNLearner):\n",
    "    \"Subclass of RNNLearner for predictions using Seq2Seq\"\n",
    "    \n",
    "    def predict(self, text:str, n_words:int=1, no_unk:bool=True, temperature:float=1., min_p:float=None, sep:str=' ',\n",
    "                decoder=decode_spec_tokens):\n",
    "        \"Return the `n_words` that come after `text`.\"\n",
    "        ## handle predictions for Seq2Seq\n",
    "        set_trace()\n",
    "        ds = self.data.single_dl.dataset\n",
    "        self.model.reset()\n",
    "        xb,yb = self.data.one_item(text)\n",
    "        new_idx = []\n",
    "        for _ in range(n_words): #progress_bar(range(n_words), leave=False):\n",
    "            res = self.pred_batch(batch=(xb,yb))[0][-1]\n",
    "            #if len(new_idx) == 0: self.model[0].select_hidden([0])\n",
    "            if no_unk: res[self.data.vocab.stoi[UNK]] = 0.\n",
    "            if min_p is not None: res[res < min_p] = 0.\n",
    "            if temperature != 1.: res.pow_(1 / temperature)\n",
    "            idx = torch.multinomial(res, 1).item()\n",
    "            new_idx.append(idx)\n",
    "            xb = xb.new_tensor([idx])[None]\n",
    "        return text + sep + sep.join(decoder(self.data.vocab.textify(new_idx, sep=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Seq2SeqLearner(data, model, **{'alpha':0,'beta':0}, callbacks=[AppendBatchTargs()], loss_func=CrossEntropyFlat())\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27,560,876'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in learn.model.parameters() if p.requires_grad)\n",
    "f'{total_params:,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in learn.model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.batch_size = 24  ## 64 fails to load.  Prob. too big embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='26316' class='' max='33232', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      79.19% [26316/33232 29:38<07:47 1.2771]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.valid_dl))\n",
    "x,y[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.model(x,y)\n",
    "preds[0][0,:].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_itos = data.label_list.train.x.vocab.itos\n",
    "y_itos = data.label_list.train.y.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(5):\n",
    "    print(' '.join([x_itos[o] for o in x[i,:] if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in y[i,:] if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in preds[0][i,:].argmax(dim=1) if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encoder(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        try: out = model.decoder(Variable(ys), memory, src_mask, Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        except: set_trace()\n",
    "        prob = F.softmax(model.out(out[:, -1]))\n",
    "\n",
    "#         next_word = torch.multinomial(prob, 1)  #sample from distribution\n",
    "#         next_word = next_word.data[0][0]\n",
    "\n",
    "        next_word = prob.argmax(dim=-1).item()  ## single best\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = x\n",
    "trg = y\n",
    "trg_input = trg[:,:-1]\n",
    "# targets = trg[:, 1:].contiguous()\n",
    "src_mask, _ = create_masks(src, trg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = greedy_decode(m, src[10].unsqueeze(0), src_mask[10], max_len=y.size(1), start_symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([x_itos[o] for o in src[10] if o != 1]))\n",
    "print(' '.join([y_itos[o] for o in trg[10] if o != 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join([y_itos[o] for o in ys[0,:] if o != 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 fasta.ai1 DEV",
   "language": "python",
   "name": "fastai1_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
