{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMT (seq2seq) in fastai v1\n",
    "\n",
    "Start with this:<br>\n",
    "https://gist.github.com/ohmeow/fe91aed6267cd779946ab9f10eccdab9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.triu(np.ones((1,5,5)), k=1).astype(int).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(5, 5), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the European Parlement Parallel Corpus<br>\n",
    "http://www.statmt.org/europarl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_es_translate_all.csv  es-en.tgz\t       europarl-v7.es-en.es\r\n",
      "en_es_translate.csv\t europarl-v7.es-en.en\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data/es-en')\n",
    "\n",
    "BASE = 'europarl-v7.es-en'\n",
    "en_file = DATA_PATH/f'{BASE}.en'\n",
    "es_file = DATA_PATH/f'{BASE}.es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = [{'english_text': e.strip(), 'spanish_text': s.strip()} \n",
    "             for e, s in zip(open(en_file, encoding='utf-8'), open(es_file, encoding='utf-8'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'english_text': 'Resumption of the session',\n",
       "  'spanish_text': 'Reanudación del período de sesiones'},\n",
       " {'english_text': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       "  'spanish_text': 'Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.'},\n",
       " {'english_text': \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
       "  'spanish_text': 'Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.'},\n",
       " {'english_text': 'You have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
       "  'spanish_text': 'Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.'},\n",
       " {'english_text': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\",\n",
       "  'spanish_text': 'A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_text</th>\n",
       "      <th>spanish_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reanudación del período de sesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Como todos han podido comprobar, el gran \"efec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Sus Señorías han solicitado un debate sobre el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>A la espera de que se produzca, de acuerdo con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_text  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                        spanish_text  \n",
       "0                Reanudación del período de sesiones  \n",
       "1  Declaro reanudado el período de sesiones del P...  \n",
       "2  Como todos han podido comprobar, el gran \"efec...  \n",
       "3  Sus Señorías han solicitado un debate sobre el...  \n",
       "4  A la espera de que se produzca, de acuerdo con...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_lines)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_PATH/'en_es_translate_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, split, build DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1568513, 2), (392128, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH/'en_es_translate_all.csv')\n",
    "df = df.dropna()  #some rows don't line up?  or there is really no translation?  \n",
    "\n",
    "rnd_seed = 20190313\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "idx = np.random.permutation(len(df))\n",
    "valid_cut = int(0.20 * len(idx))\n",
    "\n",
    "train_df = df.iloc[idx[:-valid_cut],:]\n",
    "valid_df = df.iloc[idx[-valid_cut:],:]\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_pad_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=False, \n",
    "                        backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    \n",
    "    samples = to_data(samples)\n",
    "    x_max_len = max([len(s[0]) for s in samples])\n",
    "    y_max_len = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    x_res = torch.zeros(len(samples), x_max_len).long() + pad_idx\n",
    "    y_res = torch.zeros(len(samples), y_max_len).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "        \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            x_res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            x_res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            y_res[i,:len(s[1]):] = LongTensor(s[1])\n",
    "            \n",
    "    if backwards: res = res.flip(1)\n",
    "        \n",
    "    return x_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, \n",
    "               path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1, pad_first=False, \n",
    "               device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:        \n",
    "        \"\"\"Function that transform the `datasets` in a `DataBunch` for classification.  Passes `**dl_kwargs` on to `DataLoader()`\"\"\"\n",
    "        \n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        \n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tok = Tokenizer(lang='en')\n",
    "es_tok = Tokenizer(lang='es')\n",
    "\n",
    "en_procs = [TokenizeProcessor(tokenizer=en_tok), NumericalizeProcessor()]\n",
    "es_procs = [TokenizeProcessor(tokenizer=es_tok), NumericalizeProcessor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='74' class='' max='157', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      47.13% [74/157 03:55<04:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = Path('./data/seq2seq/')\n",
    "bs = 64\n",
    "\n",
    "en_train_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['english_text'], processor=en_procs)\n",
    "es_train_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['spanish_text'], processor=es_procs)\n",
    "\n",
    "en_valid_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['english_text'])\n",
    "es_valid_il = Seq2SeqTextList.from_df(train_df, path=PATH, cols=['spanish_text'])\n",
    "\n",
    "trn_ll = LabelList(en_train_il, es_train_il)\n",
    "val_ll = LabelList(en_valid_il, es_valid_il)\n",
    "\n",
    "lls = LabelLists(PATH, train=trn_ll, valid=val_ll).process()\n",
    "\n",
    "data = lls.databunch(bs=bs, val_bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_train_il.vocab.itos), len(es_train_il.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(data.train_dl))\n",
    "b[0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.label_list.train.x.vocab.itos), len(data.label_list.train.y.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.label_list.train.x.vocab.itos), data.label_list.train.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a model for Seq2Seq\n",
    "\n",
    "Big help from this link from the old course + looking at the `Transformer` code and adapting <br>\n",
    "https://github.com/kheyer/ML-DL-Projects/blob/master/Seq2Seq%20Transformer/Transformer.ipynb\n",
    "\n",
    "and here:<br>\n",
    "https://nbviewer.jupyter.org/github/fastai/fastai/blob/6ba17b21599a6fc441794ffd130bc31b5333b4a0/courses/dl2/translate.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class m_MultiHeadAttention(nn.Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.att_q = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_k = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.att_v = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None, **kwargs):\n",
    "        \"attn -> Linear -> drop -> merge -> LN\"\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask, **kwargs))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = q.size(0),q.size(1) # bs x bptt x d_model\n",
    "        wq,wk,wv = self.att_q(q), self.att_k(k), self.att_v(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish}\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "class m_EncoderLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=m_MultiHeadAttention):\n",
    "        super().__init__()\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, x, x, mask=mask, **kwargs))\n",
    "\n",
    "class m_DecoderLayer(nn.Module):\n",
    "    \"Decoder block for seq2seq. Self and target attention combined.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=m_MultiHeadAttention):\n",
    "        super().__init__()\n",
    "        self.mhra_s    = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.mhra_targ = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "        \n",
    "    def forward(self, x:Tensor, enc_out:Tensor, src_mask:Tensor=None, targ_mask:Tensor=None, **kwargs): \n",
    "        x = self.mhra_s(x,x,x, mask=targ_mask, **kwargs)\n",
    "        return self.ff(self.mhra_targ(x, enc_out, enc_out, mask=src_mask, **kwargs))\n",
    "    \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, tgt_vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=m_MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList([m_EncoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                          ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                          attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "        self.decoder = nn.Embedding(tgt_vocab_sz, d_model)\n",
    "        self.pos_dec = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_dec = nn.Dropout(embed_p)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList([m_DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                          ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                          attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        self.tgt_word_prj = nn.Linear(d_model, tgt_vocab_sz, bias=False)\n",
    "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
    "        self.x_logit_scale = (d_model ** -0.5)\n",
    "        \n",
    "    def reset(self):pass\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        bs, x_len = x.size()\n",
    "        bs, y_len = y.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype)\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "        pos_y = torch.arange(0, y_len, device=x.device, dtype=x.dtype)\n",
    "        targ = self.drop_dec(self.decoder(y) + self.pos_dec(pos_y)[None]) #.mul_(self.d_model ** 0.5)\n",
    "\n",
    "        ## masking/padding is not yet right here.  Needs to be fixed to mask the pad IDs\n",
    "        src_mask = (x==1).byte()[:,None,None,:] #[64,5,155,155]\n",
    "        #mask == trg_mask (but trg_mask also masks out all xxpad ids [id==1]  add that here)\n",
    "        nopeak_mask = torch.triu(x.new_ones(y_len, y_len), diagonal=1).byte() if self.mask else None\n",
    "        targ_mask = (y==1).byte()[:,None,:,None] * nopeak_mask\n",
    "        \n",
    "        for layer in self.enc_layers: inp  = layer(inp, mask=src_mask)\n",
    "        for layer in self.dec_layers: targ = layer(targ, inp, src_mask=src_mask, targ_mask=targ_mask)\n",
    "        decoded = self.tgt_word_prj(targ) * self.x_logit_scale\n",
    "\n",
    "        return [decoded, decoded, decoded] #for RNN trainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfm_seq2seq = Seq2SeqTransformer(vocab_sz=len(data.label_list.train.x.vocab.itos),\n",
    "                       tgt_vocab_sz=len(data.label_list.train.y.vocab.itos),\n",
    "                       ctx_len=256, n_layers=6, n_heads=10, d_model=300, d_head=64, d_inner=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_seq2seq = Seq2SeqTransformer(vocab_sz=len(data.label_list.train.x.vocab.itos),\n",
    "                       tgt_vocab_sz=len(data.label_list.train.y.vocab.itos),\n",
    "                       ctx_len=256, n_layers=6, n_heads=10, d_model=300, d_head=None, d_inner=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transformer(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 0., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None: nn.init.normal_(m.weight, 1., 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:     nn.init.constant_(m.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tfm_seq2seq.apply(init_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AppendBatchTargs(Callback):\n",
    "    learn:Learner\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        return {'last_input':(last_input, last_target), 'last_target':last_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, tfm_seq2seq, **{'alpha':0,'beta':0}, callbacks=[AppendBatchTargs()], loss_func=CrossEntropyFlat())\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in learn.model.parameters() if p.requires_grad)\n",
    "f'{total_params:,}'  ## half the params of 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit one cycle is just too darn agressive for Transformer. Why?\n",
    "## loss going to nearly zero means we are peaking. How?\n",
    "learn.fit(25, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(learn.data.train_dl))\n",
    "\n",
    "# src = x[0,:]\n",
    "# targ = y[0,:]\n",
    "\n",
    "preds = learn.model(x,y)\n",
    "preds[0][0,:].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_itos = data.label_list.train.x.vocab.itos\n",
    "y_itos = data.label_list.train.y.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(5):\n",
    "    print(' '.join([x_itos[o] for o in x[i,:] ]))#if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in y[i,:] ]))#if o != 1]))\n",
    "    print(' '.join([y_itos[o] for o in preds[0][i,:].argmax(dim=1) ])) #if o!=1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 fasta.ai1 DEV",
   "language": "python",
   "name": "fastai1_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
